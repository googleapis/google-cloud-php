<?php
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: google/cloud/dialogflow/v2/generator.proto

namespace Google\Cloud\Dialogflow\V2;

use Google\Protobuf\Internal\GPBType;
use Google\Protobuf\Internal\RepeatedField;
use Google\Protobuf\Internal\GPBUtil;

/**
 * The parameters of inference.
 *
 * Generated from protobuf message <code>google.cloud.dialogflow.v2.InferenceParameter</code>
 */
class InferenceParameter extends \Google\Protobuf\Internal\Message
{
    /**
     * Optional. Maximum number of the output tokens for the generator.
     *
     * Generated from protobuf field <code>optional int32 max_output_tokens = 1 [(.google.api.field_behavior) = OPTIONAL];</code>
     */
    protected $max_output_tokens = null;
    /**
     * Optional. Controls the randomness of LLM predictions.
     * Low temperature = less random. High temperature = more random.
     * If unset (or 0), uses a default value of 0.
     *
     * Generated from protobuf field <code>optional double temperature = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
     */
    protected $temperature = null;
    /**
     * Optional. Top-k changes how the model selects tokens for output. A top-k of
     * 1 means the selected token is the most probable among all tokens in the
     * model's vocabulary (also called greedy decoding), while a top-k of 3 means
     * that the next token is selected from among the 3 most probable tokens
     * (using temperature). For each token selection step, the top K tokens with
     * the highest probabilities are sampled. Then tokens are further filtered
     * based on topP with the final token selected using temperature sampling.
     * Specify a lower value for less random responses and a higher value for more
     * random responses. Acceptable value is [1, 40], default to 40.
     *
     * Generated from protobuf field <code>optional int32 top_k = 3 [(.google.api.field_behavior) = OPTIONAL];</code>
     */
    protected $top_k = null;
    /**
     * Optional. Top-p changes how the model selects tokens for output. Tokens are
     * selected from most K (see topK parameter) probable to least until the sum
     * of their probabilities equals the top-p value. For example, if tokens A, B,
     * and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5,
     * then the model will select either A or B as the next token (using
     * temperature) and doesn't consider C. The default top-p value is 0.95.
     * Specify a lower value for less random responses and a higher value for more
     * random responses. Acceptable value is [0.0, 1.0], default to 0.95.
     *
     * Generated from protobuf field <code>optional double top_p = 4 [(.google.api.field_behavior) = OPTIONAL];</code>
     */
    protected $top_p = null;

    /**
     * Constructor.
     *
     * @param array $data {
     *     Optional. Data for populating the Message object.
     *
     *     @type int $max_output_tokens
     *           Optional. Maximum number of the output tokens for the generator.
     *     @type float $temperature
     *           Optional. Controls the randomness of LLM predictions.
     *           Low temperature = less random. High temperature = more random.
     *           If unset (or 0), uses a default value of 0.
     *     @type int $top_k
     *           Optional. Top-k changes how the model selects tokens for output. A top-k of
     *           1 means the selected token is the most probable among all tokens in the
     *           model's vocabulary (also called greedy decoding), while a top-k of 3 means
     *           that the next token is selected from among the 3 most probable tokens
     *           (using temperature). For each token selection step, the top K tokens with
     *           the highest probabilities are sampled. Then tokens are further filtered
     *           based on topP with the final token selected using temperature sampling.
     *           Specify a lower value for less random responses and a higher value for more
     *           random responses. Acceptable value is [1, 40], default to 40.
     *     @type float $top_p
     *           Optional. Top-p changes how the model selects tokens for output. Tokens are
     *           selected from most K (see topK parameter) probable to least until the sum
     *           of their probabilities equals the top-p value. For example, if tokens A, B,
     *           and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5,
     *           then the model will select either A or B as the next token (using
     *           temperature) and doesn't consider C. The default top-p value is 0.95.
     *           Specify a lower value for less random responses and a higher value for more
     *           random responses. Acceptable value is [0.0, 1.0], default to 0.95.
     * }
     */
    public function __construct($data = NULL) {
        \GPBMetadata\Google\Cloud\Dialogflow\V2\Generator::initOnce();
        parent::__construct($data);
    }

    /**
     * Optional. Maximum number of the output tokens for the generator.
     *
     * Generated from protobuf field <code>optional int32 max_output_tokens = 1 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @return int
     */
    public function getMaxOutputTokens()
    {
        return isset($this->max_output_tokens) ? $this->max_output_tokens : 0;
    }

    public function hasMaxOutputTokens()
    {
        return isset($this->max_output_tokens);
    }

    public function clearMaxOutputTokens()
    {
        unset($this->max_output_tokens);
    }

    /**
     * Optional. Maximum number of the output tokens for the generator.
     *
     * Generated from protobuf field <code>optional int32 max_output_tokens = 1 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @param int $var
     * @return $this
     */
    public function setMaxOutputTokens($var)
    {
        GPBUtil::checkInt32($var);
        $this->max_output_tokens = $var;

        return $this;
    }

    /**
     * Optional. Controls the randomness of LLM predictions.
     * Low temperature = less random. High temperature = more random.
     * If unset (or 0), uses a default value of 0.
     *
     * Generated from protobuf field <code>optional double temperature = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @return float
     */
    public function getTemperature()
    {
        return isset($this->temperature) ? $this->temperature : 0.0;
    }

    public function hasTemperature()
    {
        return isset($this->temperature);
    }

    public function clearTemperature()
    {
        unset($this->temperature);
    }

    /**
     * Optional. Controls the randomness of LLM predictions.
     * Low temperature = less random. High temperature = more random.
     * If unset (or 0), uses a default value of 0.
     *
     * Generated from protobuf field <code>optional double temperature = 2 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @param float $var
     * @return $this
     */
    public function setTemperature($var)
    {
        GPBUtil::checkDouble($var);
        $this->temperature = $var;

        return $this;
    }

    /**
     * Optional. Top-k changes how the model selects tokens for output. A top-k of
     * 1 means the selected token is the most probable among all tokens in the
     * model's vocabulary (also called greedy decoding), while a top-k of 3 means
     * that the next token is selected from among the 3 most probable tokens
     * (using temperature). For each token selection step, the top K tokens with
     * the highest probabilities are sampled. Then tokens are further filtered
     * based on topP with the final token selected using temperature sampling.
     * Specify a lower value for less random responses and a higher value for more
     * random responses. Acceptable value is [1, 40], default to 40.
     *
     * Generated from protobuf field <code>optional int32 top_k = 3 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @return int
     */
    public function getTopK()
    {
        return isset($this->top_k) ? $this->top_k : 0;
    }

    public function hasTopK()
    {
        return isset($this->top_k);
    }

    public function clearTopK()
    {
        unset($this->top_k);
    }

    /**
     * Optional. Top-k changes how the model selects tokens for output. A top-k of
     * 1 means the selected token is the most probable among all tokens in the
     * model's vocabulary (also called greedy decoding), while a top-k of 3 means
     * that the next token is selected from among the 3 most probable tokens
     * (using temperature). For each token selection step, the top K tokens with
     * the highest probabilities are sampled. Then tokens are further filtered
     * based on topP with the final token selected using temperature sampling.
     * Specify a lower value for less random responses and a higher value for more
     * random responses. Acceptable value is [1, 40], default to 40.
     *
     * Generated from protobuf field <code>optional int32 top_k = 3 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @param int $var
     * @return $this
     */
    public function setTopK($var)
    {
        GPBUtil::checkInt32($var);
        $this->top_k = $var;

        return $this;
    }

    /**
     * Optional. Top-p changes how the model selects tokens for output. Tokens are
     * selected from most K (see topK parameter) probable to least until the sum
     * of their probabilities equals the top-p value. For example, if tokens A, B,
     * and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5,
     * then the model will select either A or B as the next token (using
     * temperature) and doesn't consider C. The default top-p value is 0.95.
     * Specify a lower value for less random responses and a higher value for more
     * random responses. Acceptable value is [0.0, 1.0], default to 0.95.
     *
     * Generated from protobuf field <code>optional double top_p = 4 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @return float
     */
    public function getTopP()
    {
        return isset($this->top_p) ? $this->top_p : 0.0;
    }

    public function hasTopP()
    {
        return isset($this->top_p);
    }

    public function clearTopP()
    {
        unset($this->top_p);
    }

    /**
     * Optional. Top-p changes how the model selects tokens for output. Tokens are
     * selected from most K (see topK parameter) probable to least until the sum
     * of their probabilities equals the top-p value. For example, if tokens A, B,
     * and C have a probability of 0.3, 0.2, and 0.1 and the top-p value is 0.5,
     * then the model will select either A or B as the next token (using
     * temperature) and doesn't consider C. The default top-p value is 0.95.
     * Specify a lower value for less random responses and a higher value for more
     * random responses. Acceptable value is [0.0, 1.0], default to 0.95.
     *
     * Generated from protobuf field <code>optional double top_p = 4 [(.google.api.field_behavior) = OPTIONAL];</code>
     * @param float $var
     * @return $this
     */
    public function setTopP($var)
    {
        GPBUtil::checkDouble($var);
        $this->top_p = $var;

        return $this;
    }

}

